{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd1a01c5-7f57-49fa-8759-a9289675150a",
   "metadata": {},
   "source": [
    "# Quickstart Guide: OpenAI\n",
    "Langchain is still changing rapidly. \n",
    "See [quickstart](https://python.langchain.com/docs/get_started/quickstart) for up-to-date instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb07ab-638c-4c46-a922-1b15dc7408cc",
   "metadata": {},
   "source": [
    "Set OPENAI_API_KEY within .env before running this step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "651e51a5-6e8d-4998-8f40-6aadebd214ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921cadfc-799d-4c3d-9c46-e76b2a16ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dddec0-438d-481a-aa82-c02bb575c0b1",
   "metadata": {},
   "source": [
    "Because langsmith is new, the llm will not give a satisfactory answer without additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92252f21-065c-4ced-a566-fc87674fc1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langsmith can help with testing in several ways:\\n\\n1. Test Automation: Langsmith can generate test scripts and automate the execution of test cases. It can create a variety of test scenarios with different inputs and expected outputs, saving time and effort in manual testing.\\n\\n2. Test Data Generation: Langsmith can generate synthetic test data that covers a wide range of inputs, allowing for extensive and comprehensive testing. This helps uncover potential bugs and edge cases that might not be identified with limited real-world data.\\n\\n3. Test Case Prioritization: Langsmith can analyze the codebase and identify critical areas that require more attention in testing. It can assist in prioritizing test cases based on code complexity, code coverage, and potential impact, ensuring that the most crucial parts of the application are thoroughly tested.\\n\\n4. Continuous Testing: Langsmith can be integrated into the Continuous Integration/Continuous Delivery (CI/CD) pipeline to automatically test code changes as they are made. This helps catch bugs early in the development cycle and ensures that new features or updates do not introduce regressions.\\n\\n5. Performance Testing: Langsmith can simulate various user loads and stress test the system to evaluate its performance under different conditions. It can generate realistic user behavior, allowing for the identification of potential performance bottlenecks and scalability issues.\\n\\n6. Security Testing: Langsmith can generate test cases that target security vulnerabilities, such as SQL injection, cross-site scripting, or authentication bypass. This helps identify potential security weaknesses in the application and ensure that it meets the required security standards.\\n\\n7. Exploratory Testing: Langsmith can assist testers in exploring the application by generating random test scenarios. It can help uncover unexpected behavior, errors, and usability issues that might not be covered by scripted test cases.\\n\\nOverall, Langsmith's capabilities in test automation, test data generation, test case prioritization, continuous testing, performance testing, security testing, and exploratory testing make it a valuable tool for improving the efficiency and effectiveness of the testing process.\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abd6b49-3f57-4e55-9280-fa52ecb5a8a1",
   "metadata": {},
   "source": [
    "^ It is totally BSing and has no idea we are talking about a langchain-related utility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c1023e-c728-4aca-bf8e-52b8efe1a6c9",
   "metadata": {},
   "source": [
    "Using a prompt template..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be958f1-3ba0-462b-9d96-2e173f354c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626be4c-ca76-425c-8067-d2d1b565ba36",
   "metadata": {},
   "source": [
    "With LCEL piping syntax (new recommended syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a527973-851d-4d84-ad2d-200334b91cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langsmith can be a valuable tool for testing in a variety of ways. Here are some ways in which Langsmith can assist with testing:\\n\\n1. Test Data Generation: Langsmith can generate realistic and diverse test data automatically. It can create synthetic data that simulates real-world scenarios, helping to uncover edge cases and validate the functionality of your software.\\n\\n2. Test Case Generation: Langsmith can automatically generate test cases based on your specifications or requirements. It can analyze your codebase and generate test cases that cover different paths and conditions, reducing the manual effort required for test case creation.\\n\\n3. Test Automation: Langsmith can integrate with popular testing frameworks and tools, allowing you to automate the execution of test cases. This saves time and effort by eliminating the need for manual execution of repetitive tests.\\n\\n4. Performance Testing: Langsmith can simulate different user loads and stress test your application to identify performance bottlenecks. It can generate realistic traffic and measure response times to help you optimize your application's performance.\\n\\n5. Security Testing: Langsmith can assist in identifying security vulnerabilities by generating and executing various attack scenarios. It can simulate different types of attacks, such as SQL injection or cross-site scripting, to help you identify and fix potential security issues.\\n\\n6. Regression Testing: Langsmith can speed up regression testing by automatically generating and executing test cases. It can ensure that new features or bug fixes do not introduce regressions by retesting previously validated functionality.\\n\\n7. Exploratory Testing: Langsmith can provide suggestions for test scenarios based on its analysis of your codebase. It can help testers explore different paths and functionalities, ensuring comprehensive test coverage.\\n\\n8. Test Reporting and Documentation: Langsmith can generate detailed reports and documentation about the tests executed, including test coverage, test results, and any issues encountered. This documentation can be useful for tracking the progress of testing efforts and sharing information with stakeholders.\\n\\nOverall, Langsmith can significantly improve the efficiency, accuracy, and coverage of your testing efforts, helping you deliver high-quality software.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd7cc9-a578-4823-b56f-6138e67453fb",
   "metadata": {},
   "source": [
    "Adding an output parser to the end of the pipeline to grab strings instead of objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c0f403-fa08-40d7-b386-c2c4f0576b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Langsmith can greatly assist with testing by providing a powerful and versatile language generation capability. Here are a few ways in which Langsmith can help:\\n\\n1. Test Data Generation: Langsmith can generate a wide range of test data, including realistic names, addresses, dates, phone numbers, and more. This can be particularly useful for populating test databases or creating test scenarios with different data variations.\\n\\n2. Test Scenario Generation: With Langsmith, you can generate complex and diverse test scenarios by automatically generating natural language descriptions of test cases. This can save significant time and effort in manually creating test scenarios.\\n\\n3. Automated Test Script Generation: Langsmith's language generation capabilities can be leveraged to automatically generate test scripts in various programming languages. By providing input parameters and expected outputs, Langsmith can generate the code required to execute the test cases.\\n\\n4. Test Documentation: Langsmith can be used to automatically generate test documentation, including test plans, test cases, and test reports. This ensures that the documentation remains up-to-date and consistent with the actual test scenarios being executed.\\n\\n5. User Interface Testing: Langsmith can generate natural language instructions for testing user interfaces. This can be particularly helpful for usability testing, as it allows testers to focus on the user experience rather than getting caught up in technical details.\\n\\n6. Localization Testing: Langsmith can assist in generating localized content for testing internationalization and localization of software. By generating text in different languages and cultural contexts, Langsmith helps ensure that the software functions correctly in diverse linguistic and cultural settings.\\n\\nOverall, Langsmith's language generation capabilities can significantly enhance the efficiency and effectiveness of testing processes, enabling testers to focus on critical aspects of software quality assurance.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c37feee-7970-4fb9-9f5d-3ceea02f4805",
   "metadata": {},
   "source": [
    "## Adding retrieval chain\n",
    "Use langchains tools to add a vectorstore for providing langchain-specific context for langsmith."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1281c37d-ddb9-4afc-bc75-942a7a43c9d7",
   "metadata": {},
   "source": [
    "Make sure beautifulsoup4 is installed into the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb2dd875-3215-4052-9894-12ea166db02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d105bd90-f1ad-49b9-b813-7054b085cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2d5c29-6d3c-4a97-9e88-667c8e07480a",
   "metadata": {},
   "source": [
    "Make sure a FAISS lib is installed before next step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f93c4704-7dee-48a8-8654-4fdc0fc6f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ab70e-da63-4e40-a0e3-81ba7bec7b6f",
   "metadata": {},
   "source": [
    "With the vectorstore populated, build the retrieval chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e2584b-2273-4195-8089-16ace6a3cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37063c16-3a1a-4b4f-9b47-acf4503ed47d",
   "metadata": {},
   "source": [
    "Could be run by passing directly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf95e851-9d4c-47a5-80f8-d102528067b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can help with testing by allowing you to visualize test results.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with testing?\",\n",
    "    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5141c11-c22d-476e-9fbb-0d85f8398ce7",
   "metadata": {},
   "source": [
    "But to get the full context, use a retreival chain to pull from the vectorstore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24790f8f-3675-4904-8a37-aed1f65805f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing by providing a way to construct datasets and evaluate the performance of prompts or chains. It allows users to upload datasets and run chains over the data points, logging the results to a new project associated with the dataset. Users can review these results and assign feedback to runs, marking them as correct or incorrect. Additionally, LangSmith provides evaluators that can be specified during a test run to evaluate the results. While these evaluators may not be perfect, they can guide users to examples that should be further examined. Furthermore, LangSmith offers annotation queues for manual review and annotation of runs, allowing users to assess subjective qualities that automatic evaluators struggle with and validate auto-evaluated runs.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2702faac-d793-4abf-a70b-7051bd55e5bf",
   "metadata": {},
   "source": [
    "Now it is talking about the real langsmith."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0742b5-5f53-4f78-a219-d3e25134d1dd",
   "metadata": {},
   "source": [
    "## Conversation Retrieval Chain\n",
    "Adding cross-message memory for chatbots by feeding back the chat history as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64f8c7ed-e06a-461a-bf19-67f7079c35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4f762-a67f-473c-a862-b8c377b99862",
   "metadata": {},
   "source": [
    "Test by asking a follow-up question that assumes awareness of a prior context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0f61a77-8d5f-4260-a15b-b617d7366b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmith’s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don’t even look at the traces, but the 10% of the time that we do… it’s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string → string (or chat messages → chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We’ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing — mirroring the debug mode approach.We’ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these aren’t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.↩PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we’ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we’re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb08b3-cb5e-44ae-b4d6-f41e57cb10de",
   "metadata": {},
   "source": [
    "The objects returned are query results relevant to the question from the vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e1fe702-15ab-4afb-819d-632ae6009e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4731b679-3eb9-441a-93bc-c5e2ad136ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help test your LLM applications by providing features that simplify the testing process. Here are some ways LangSmith supports testing:\n",
      "\n",
      "1. Dataset Uploading: LangSmith simplifies dataset uploading, allowing you to easily upload and manage datasets for testing.\n",
      "\n",
      "2. Running Chains Over Data Points: Once you have a dataset, you can run chains over the data points using the LangSmith client. This allows you to test your prompt or chain by visualizing the outputs. The results can be logged to a new project associated with the dataset for later review.\n",
      "\n",
      "3. Feedback and Evaluation: LangSmith makes it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app. You can review and evaluate the results, and LangSmith provides aggregate statistics for each test project.\n",
      "\n",
      "4. Evaluators: LangSmith provides a set of evaluators in the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. While these evaluators are not perfect, they can guide your attention to examples that require manual review.\n",
      "\n",
      "5. Human Evaluation: LangSmith allows for manual review and annotation of runs through annotation queues. You can select runs based on criteria like model type or automatic evaluation scores and queue them up for human review. As a reviewer, you can quickly step through the runs, view inputs, outputs, and existing tags, and provide your own feedback.\n",
      "\n",
      "By utilizing these features, LangSmith helps streamline the testing process for your LLM applications, allowing you to assess quality, reliability, and performance.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "response = retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c86aa5-f141-418e-a192-95146936ab8e",
   "metadata": {},
   "source": [
    "## Agent\n",
    "Let the LLM decide and take action using a list of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8a913ed-8e7c-489c-ab18-4ed569564302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb9225-a527-45fe-a4b9-d16adce5a156",
   "metadata": {},
   "source": [
    "Make sure TAVILY_API_KEY is setup in .env before next step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84abdbf6-d4a4-4ce5-a0a3-1cc17bade11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "load_dotenv()\n",
    "search = TavilySearchResults()\n",
    "tools = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b40bab0-dada-42ad-9249-1db109b2433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "023aed46-baf4-4044-8f05-138fec8aab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangSmith can help with testing in several ways:\n",
      "\n",
      "1. Test Case Generation: LangSmith can generate test cases automatically based on the specifications of the software being tested. It uses advanced techniques such as symbolic execution and constraint solving to generate test inputs that cover different execution paths and edge cases.\n",
      "\n",
      "2. Test Oracles: LangSmith can help in defining test oracles, which are the expected outputs or behaviors of the software under test. It can analyze the specifications and requirements of the software to automatically generate assertions or checks that can be used as test oracles.\n",
      "\n",
      "3. Test Coverage Analysis: LangSmith can analyze the code coverage of the test cases to determine which parts of the software have been exercised during testing. This helps in identifying areas of the code that have not been tested and may require additional test cases.\n",
      "\n",
      "4. Test Prioritization: LangSmith can prioritize the execution of test cases based on their likelihood of finding bugs or their impact on the system. It uses techniques such as mutation analysis and fault localization to identify the most critical test cases that should be executed first.\n",
      "\n",
      "5. Test Result Analysis: LangSmith can analyze the results of the test cases to identify patterns or trends that may indicate the presence of bugs or performance issues. It can also help in debugging and diagnosing the root cause of failures.\n",
      "\n",
      "Overall, LangSmith provides automated and intelligent support for various aspects of testing, helping testers to improve the effectiveness and efficiency of their testing efforts.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how can langsmith help with testing?',\n",
       " 'output': 'LangSmith can help with testing in several ways:\\n\\n1. Test Case Generation: LangSmith can generate test cases automatically based on the specifications of the software being tested. It uses advanced techniques such as symbolic execution and constraint solving to generate test inputs that cover different execution paths and edge cases.\\n\\n2. Test Oracles: LangSmith can help in defining test oracles, which are the expected outputs or behaviors of the software under test. It can analyze the specifications and requirements of the software to automatically generate assertions or checks that can be used as test oracles.\\n\\n3. Test Coverage Analysis: LangSmith can analyze the code coverage of the test cases to determine which parts of the software have been exercised during testing. This helps in identifying areas of the code that have not been tested and may require additional test cases.\\n\\n4. Test Prioritization: LangSmith can prioritize the execution of test cases based on their likelihood of finding bugs or their impact on the system. It uses techniques such as mutation analysis and fault localization to identify the most critical test cases that should be executed first.\\n\\n5. Test Result Analysis: LangSmith can analyze the results of the test cases to identify patterns or trends that may indicate the presence of bugs or performance issues. It can also help in debugging and diagnosing the root cause of failures.\\n\\nOverall, LangSmith provides automated and intelligent support for various aspects of testing, helping testers to improve the effectiveness and efficiency of their testing efforts.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89427527-3354-4d37-9924-7c82dba98c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'weather in Austin TX'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'url': 'https://www.kxan.com/news/local/austin/austin-2024-weather-data/', 'content': 'The chart below shows the high temperature recorded in Austin each day in 2024.  Temperatures, rain, severe storms: Explore Austin’s 2024 weather data by: Christopher Adams  Hourly Forecast Austin-Travis County City continues preparing for ice storms in 2024  Austin’s high temperature in 2023 was 110°, on Aug. 17. The low temperature for the year was 29°, set on both Jan. 3173° #1: 73° (Jan 11) #2: 70° (Jan 8) #3: 67° (Jan 12) #4: 67° (Jan 10) #5: 64° (Jan 7 +1 other) Coldest Low 16° #1: 16° (Jan 17) #2: 16° (Jan 15) #3: 17° (Jan 16) #4: 17° (Jan 14) #5: 32° (Jan 10)...'}, {'url': 'https://en.climate-data.org/north-america/united-states-of-america/texas/austin-714914/t/january-1/', 'content': 'all information about the weather in Austin in January:  Austin weather in January Austin weather by month // weather averages 10.4 (50.7) 6.1 (43) 16.6 (61.8) 64 (2.5) 12.5  Data: 1999 - 2019: avg. Sun hours Austin weather and climate for further months  Are you planning a holiday with hopefully nice weather in Austin in January 2024? Here you can find all informationData: 1991 - 2021 Min. Temperature °C (°F), Max. Temperature °C (°F), Precipitation / Rainfall mm (in), Humidity, Rainy days. Data: 1999 - 2019: avg. Sun hours Austin weather and climate for further months Austin in February Austin in March Austin in April Austin in May Austin in June Austin in July Austin in August Austin in September'}, {'url': 'https://www.statesman.com/story/news/local/2024/01/24/road-closures-texas-map-txdox-austin-weather/72337375007/', 'content': \"What's the weather in Austin this week?  Rain, thunderstorms close more than 90 roads in Central Texas. See weather forecast  show a cool and rainless start to next week, with lows in the 30s and highs in the 60s.  More:See live map of road conditions, closures in Central Texas as excessive rainfall hitsRain, thunderstorms close more than 90 roads in Central Texas. See weather forecast. A deluge of rain over the course of three days has brought a cumulative 3.73 inches of rain at Camp Mabry, with ...\"}, {'url': 'https://www.statesman.com/story/news/local/2024/01/25/rainfall-totals-texas-last-96-hours-national-weather-service-floods-thunderstorms/72353547007/', 'content': 'on the amount of rain cities in Central Texas have gotten in the past 96 hours.  PHOTOS:See flooding and road closures in Austin as heavy rain hits Central Texas San Marcos: 3.8 to 7.5 inches  Rain, rain go away:Flooding prompts over 80 road closures from Williamson to Hays counties Austin: 4-5 inches  How much rain did Central Texas get over past 96 hours?San Antonio: 3.1 to 5.8 inches. Rain totals in San Antonio spanned from almost 3.1 inches to 5.8 inches in the past 96 hours, according to the weather service. PHOTOS: See flooding and road ...'}]\u001b[0m\u001b[32;1m\u001b[1;3mI found some information about the weather in Austin, TX. Here are a few sources you can check for more details:\n",
      "\n",
      "1. [KXAN News](https://www.kxan.com/news/local/austin/austin-2024-weather-data/): This article provides a chart showing the high temperatures recorded in Austin each day in 2024.\n",
      "\n",
      "2. [Climate Data](https://en.climate-data.org/north-america/united-states-of-america/texas/austin-714914/t/january-1/): This website offers information about the weather in Austin in January, including average temperatures, precipitation, and humidity.\n",
      "\n",
      "3. [Statesman](https://www.statesman.com/story/news/local/2024/01/24/road-closures-texas-map-txdox-austin-weather/72337375007/): This article discusses road closures in Central Texas due to rain and thunderstorms. It also mentions the weather forecast for the upcoming week, with cool temperatures and no rain.\n",
      "\n",
      "4. [Statesman](https://www.statesman.com/story/news/local/2024/01/25/rainfall-totals-texas-last-96-hours-national-weather-service-floods-thunderstorms/72353547007/): This article provides rainfall totals for Central Texas in the past 96 hours, including Austin, San Marcos, and San Antonio.\n",
      "\n",
      "Please note that the information provided may vary depending on the source and the specific date you are interested in.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the weather in Austin TX?',\n",
       " 'output': 'I found some information about the weather in Austin, TX. Here are a few sources you can check for more details:\\n\\n1. [KXAN News](https://www.kxan.com/news/local/austin/austin-2024-weather-data/): This article provides a chart showing the high temperatures recorded in Austin each day in 2024.\\n\\n2. [Climate Data](https://en.climate-data.org/north-america/united-states-of-america/texas/austin-714914/t/january-1/): This website offers information about the weather in Austin in January, including average temperatures, precipitation, and humidity.\\n\\n3. [Statesman](https://www.statesman.com/story/news/local/2024/01/24/road-closures-texas-map-txdox-austin-weather/72337375007/): This article discusses road closures in Central Texas due to rain and thunderstorms. It also mentions the weather forecast for the upcoming week, with cool temperatures and no rain.\\n\\n4. [Statesman](https://www.statesman.com/story/news/local/2024/01/25/rainfall-totals-texas-last-96-hours-national-weather-service-floods-thunderstorms/72353547007/): This article provides rainfall totals for Central Texas in the past 96 hours, including Austin, San Marcos, and San Antonio.\\n\\nPlease note that the information provided may vary depending on the source and the specific date you are interested in.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is the weather in Austin TX?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4baef134-128a-4bec-93ce-7355de56ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangSmith can help test your LLM applications in the following ways:\n",
      "\n",
      "1. Personal Statement Review: LangSmith can provide feedback and suggestions on your personal statement to ensure it effectively communicates your motivations, experiences, and goals. This can help you make a strong impression on the admissions committee.\n",
      "\n",
      "2. Resume/CV Review: LangSmith can review your resume or CV and provide recommendations on how to highlight your relevant experiences, skills, and achievements. This can help you present a compelling case for your admission.\n",
      "\n",
      "3. Essay Editing: LangSmith can assist with editing and proofreading your application essays, ensuring they are well-written, coherent, and free of grammatical errors. This can enhance the overall quality of your application.\n",
      "\n",
      "4. Interview Preparation: LangSmith can provide guidance and practice for your LLM interviews. This can include mock interviews, feedback on your responses, and tips on how to effectively communicate your qualifications and motivations.\n",
      "\n",
      "5. School Selection: LangSmith can help you research and evaluate different LLM programs to find the ones that best align with your interests, career goals, and academic background. This can ensure that you choose the right schools to apply to.\n",
      "\n",
      "Please note that while LangSmith can provide valuable assistance and guidance, it is important to also consult with academic advisors, mentors, and other professionals in the field to ensure a comprehensive and well-rounded application.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how',\n",
       " 'output': 'LangSmith can help test your LLM applications in the following ways:\\n\\n1. Personal Statement Review: LangSmith can provide feedback and suggestions on your personal statement to ensure it effectively communicates your motivations, experiences, and goals. This can help you make a strong impression on the admissions committee.\\n\\n2. Resume/CV Review: LangSmith can review your resume or CV and provide recommendations on how to highlight your relevant experiences, skills, and achievements. This can help you present a compelling case for your admission.\\n\\n3. Essay Editing: LangSmith can assist with editing and proofreading your application essays, ensuring they are well-written, coherent, and free of grammatical errors. This can enhance the overall quality of your application.\\n\\n4. Interview Preparation: LangSmith can provide guidance and practice for your LLM interviews. This can include mock interviews, feedback on your responses, and tips on how to effectively communicate your qualifications and motivations.\\n\\n5. School Selection: LangSmith can help you research and evaluate different LLM programs to find the ones that best align with your interests, career goals, and academic background. This can ensure that you choose the right schools to apply to.\\n\\nPlease note that while LangSmith can provide valuable assistance and guidance, it is important to also consult with academic advisors, mentors, and other professionals in the field to ensure a comprehensive and well-rounded application.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "agent_executor.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a903ee-c85d-485c-b0c0-84bb6aa40690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
